{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "11c6215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertConfig, BertModel, BertTokenizer\n",
    "from transformers.models.bert.modeling_bert import BertLayer, BertEncoder\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "93f6fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_masking(scores, keep_ratio):\n",
    "    \"\"\"\n",
    "    Create a hard mask by keeping the top-k tokens based on scores.\n",
    "    Args:\n",
    "        scores (torch.Tensor): Scores for each token (batch_size, seq_len).\n",
    "        keep_ratio (float): Ratio of tokens to keep (between 0 and 1).\n",
    "    Returns:\n",
    "        torch.Tensor: Hard mask (batch_size, seq_len) with 1s for kept tokens and 0s for pruned tokens.\n",
    "    \"\"\"\n",
    "    _, seq_len = scores.size()\n",
    "    k = int(seq_len * keep_ratio)\n",
    "\n",
    "    # Get the top-k indices\n",
    "    topk_indices = torch.topk(scores, k, dim=-1).indices\n",
    "\n",
    "    # Create a mask initialized to zeros\n",
    "    mask = torch.zeros_like(scores)\n",
    "\n",
    "    # Scatter 1s into the mask at the top-k indices\n",
    "    mask.scatter_(1, topk_indices, 1)\n",
    "\n",
    "    return mask\n",
    "\n",
    "def magnitude_head_scores(attention_output, num_heads):\n",
    "    \"\"\"\n",
    "    attention_output: (batch, seq_len, hidden_size)\n",
    "    returns: (batch, num_heads)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, hidden_size = attention_output.size()\n",
    "    head_dim = hidden_size // num_heads\n",
    "\n",
    "    # E \\in (batch, heads, L0, D)\n",
    "    E = attention_output.view(\n",
    "        batch_size, seq_len, num_heads, head_dim\n",
    "    ).permute(0, 2, 1, 3)\n",
    "\n",
    "    # s_h = sum_{l,d} |E|\n",
    "    head_scores = E.abs().sum(dim=(2, 3))  # (batch, heads)\n",
    "\n",
    "    # print(\"Raw head scores (summed magnitudes):\", head_scores.cpu().detach()/seq_len)\n",
    "    return head_scores/seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b4208ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CascadingMaskBertLayer(BertLayer):\n",
    "    def __init__(self, config: BertConfig, prune_token_percent, prune_head_percent, visualize=False):\n",
    "        super().__init__(config)\n",
    "        self.prune_token_percent = prune_token_percent\n",
    "        self.prune_head_percent = prune_head_percent\n",
    "        self.visualize = visualize\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        token_mask=None,\n",
    "        head_mask=None,\n",
    "        output_attentions=True,\n",
    "    ):\n",
    "        if token_mask is None:\n",
    "            token_mask = torch.ones(\n",
    "                hidden_states.size()[:-1],\n",
    "                device=hidden_states.device\n",
    "            )\n",
    "\n",
    "        batch_size = hidden_states.size(0)\n",
    "        num_heads = self.attention.self.num_attention_heads\n",
    "\n",
    "        if head_mask is None:\n",
    "            head_mask = torch.ones(\n",
    "                (batch_size, num_heads),\n",
    "                device=hidden_states.device\n",
    "            )\n",
    "\n",
    "        head_mask_expanded = head_mask[:, :, None, None]\n",
    "\n",
    "        # ---- Apply previous cascade ----\n",
    "        hidden_states = hidden_states * token_mask.unsqueeze(-1)\n",
    "\n",
    "        # ---- Extend attention mask ----\n",
    "        cascade_attn_mask = (1.0 - token_mask) * -1e4\n",
    "        attention_mask = attention_mask + cascade_attn_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # ---- Self-attention ----\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask=head_mask_expanded,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attention_output, attention_scores = self_attention_outputs\n",
    "\n",
    "        if self.visualize:\n",
    "            for sample in range(attention_scores.size(0)):\n",
    "                if head_mask_expanded[sample,0,0,0] == 0:  # Check if head 0 is active for this sample\n",
    "                    print(f\"Sample {sample} head 0 is pruned, skipping attention score visualization.\")\n",
    "                    continue\n",
    "                plt.figure()\n",
    "                plt.title(f\"Attention scores for sample {sample} (head 0):\")\n",
    "                sns.heatmap(attention_scores[sample,0,:,:].cpu().detach(), cmap='viridis')\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "            plt.figure()\n",
    "            plt.title(f\"Head mask for each sample\")\n",
    "            # sns.heatmap(attention_scores.sum(dim=(2,3)).cpu().detach(), cmap='viridis')\n",
    "            sns.heatmap(head_mask, cmap='viridis')\n",
    "            plt.ylabel(\"Sample index\")\n",
    "            plt.xlabel(\"Head index\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        # ---- Compute new token decisions ----\n",
    "        token_scores = attention_scores.sum(dim=(1,2))  # (batch_size, seq_len)\n",
    "\n",
    "        new_token_mask = topk_masking(\n",
    "            token_scores,\n",
    "            keep_ratio=1-self.prune_token_percent  # eliminate bottom pt% tokens\n",
    "        )\n",
    "        # Protect CLS\n",
    "        new_token_mask[:, 0] = 1.0\n",
    "                \n",
    "        # ---- CASCADE ----\n",
    "        token_mask = token_mask * new_token_mask\n",
    "\n",
    "\n",
    "\n",
    "        # ---- Compute new head decisions ----\n",
    "        heads_scores = magnitude_head_scores(attention_output, num_heads=num_heads)\n",
    "        \n",
    "        new_head_mask = topk_masking(\n",
    "            heads_scores,  # (batch_size, num_heads)\n",
    "            keep_ratio=1-self.prune_head_percent\n",
    "        )\n",
    "\n",
    "        # ---- CASCADE ----\n",
    "        head_mask = head_mask * new_head_mask\n",
    "\n",
    "\n",
    "\n",
    "        # ---- Feed-forward ----\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "\n",
    "        return layer_output, token_mask, head_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "50b11f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CascadingBertEncoder(BertEncoder):\n",
    "    def __init__(self, config, visualize, visualize_prune_decisions=False):\n",
    "        super().__init__(config)\n",
    "        self.layer = nn.ModuleList([\n",
    "            CascadingMaskBertLayer(config, config.pt[i], config.ph[i], visualize=visualize[i])\n",
    "            for i in range(config.num_hidden_layers)\n",
    "        ])\n",
    "        self.visualize_prune_decisions = visualize_prune_decisions\n",
    "\n",
    "        # for i in range(len(self.bert.encoder.layer)):\n",
    "        #     self.bert.encoder.layer[i] = CascadingMaskBertLayer(self.bert.config, pt_schedule[i], ph_schedule[i])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        batch_size = hidden_states.size(0)\n",
    "        token_mask = None\n",
    "        head_mask = None\n",
    "        \n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            hidden_states, token_mask, head_mask = layer_module(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                token_mask=token_mask,\n",
    "                head_mask=head_mask,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "\n",
    "            if self.visualize_prune_decisions:\n",
    "                print(f\"Layer {i} active tokens:\", token_mask.sum(dim=1))\n",
    "                print(f\"Layer {i} active heads:\", head_mask.sum(dim=1))\n",
    "        \n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    None,\n",
    "                    None,\n",
    "                    None,\n",
    "                    None,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=None,\n",
    "            hidden_states=None,\n",
    "            attentions=None,\n",
    "            cross_attentions=None,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c7ca9f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cascading token pruning schedule: [0.0, 0.09999999999999998, 0.18999999999999995, 0.2709999999999999, 0.3438999999999999, 0.4095099999999998, 0.46855899999999984, 0.5217030999999999, 0.5695327899999998, 0.6125795109999999, 0.6513215598999998, 0.6861894039099998]\n",
      "Cascading head pruning schedule: [0.0, 0.09999999999999998, 0.18999999999999995, 0.2709999999999999, 0.3438999999999999, 0.4095099999999998, 0.46855899999999984, 0.5217030999999999, 0.5695327899999998, 0.6125795109999999, 0.6513215598999998, 0.6861894039099998]\n",
      "Output shape: tensor([[-0.4968,  0.7481],\n",
      "        [-0.0503,  0.7157]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cfg = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "cfg.output_attentions = True\n",
    "cfg.output_hidden_states = True\n",
    "cfg.return_dict = True\n",
    "\n",
    "pt_schedule = [0.1 for _ in range(cfg.num_hidden_layers)]\n",
    "pt_schedule[0] = 0.0  # No pruning in the first layer\n",
    "\n",
    "ph_schedule = [0.1 for _ in range(cfg.num_hidden_layers)]\n",
    "ph_schedule[0] = 0.0  # No pruning in the first layer\n",
    "\n",
    "def calculate_p_schedule(p_schedule):\n",
    "    for i in range(len(p_schedule)):\n",
    "        if i == 0:\n",
    "            p_schedule[i] = 0.0\n",
    "        else:\n",
    "            p_schedule[i] = 1-(1-p_schedule[i-1])*(1-p_schedule[i])\n",
    "\n",
    "    return p_schedule\n",
    "\n",
    "\n",
    "cfg.pt = calculate_p_schedule(pt_schedule)\n",
    "print(\"Cascading token pruning schedule:\", cfg.pt)\n",
    "\n",
    "cfg.ph = calculate_p_schedule(ph_schedule)\n",
    "print(\"Cascading head pruning schedule:\", cfg.ph)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sentence = [\n",
    "    \"This is a sample input for the BERT model with cascading token pruning.\",\n",
    "    \"It demonstrates how tokens are pruned layer by layer based on attention scores.\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "class MyClassifier(nn.Module):\n",
    "    def __init__(self, config:BertConfig):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "        self.bert.encoder = CascadingBertEncoder(self.bert.config, visualize=[False for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "        self.dense = nn.Linear(config.hidden_size, 2)  # Binary classification\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        bert_outputs = self.bert(**inputs)\n",
    "\n",
    "        # print(\"BERT output:\", bert_outputs)\n",
    "        cls_token = bert_outputs.last_hidden_state[:, 0]  # Use [CLS] token representation\n",
    "        return self.dense(cls_token)\n",
    "    \n",
    "myclassifier = MyClassifier(cfg)\n",
    "outputs = myclassifier(**inputs)\n",
    "print(\"Output shape:\", outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf436e62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
