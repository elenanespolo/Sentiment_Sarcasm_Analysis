{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import transformers\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dataset.besstie import dataset_besstie\n",
    "\n",
    "root_folder = \"dataset/besstie/\"\n",
    "splits = {'train': 'train.csv', 'validation': 'valid.csv'}\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "if not os.path.exists(os.path.join(root_folder, splits[\"train\"])) or not os.path.exists(os.path.join(root_folder, splits[\"validation\"])):\n",
    "    print(\"Downloading BESSTIE dataset...\")\n",
    "    # Login using e.g. `huggingface-cli login` to access this dataset\n",
    "    df = pd.read_csv(\"hf://datasets/unswnlporg/BESSTIE/\" + splits[\"train\"])\n",
    "    df.to_csv(os.path.join(root_folder, splits[\"train\"]), index=False)\n",
    "    df = pd.read_csv(\"hf://datasets/unswnlporg/BESSTIE/\" + splits[\"validation\"])\n",
    "    df.to_csv(os.path.join(root_folder, splits[\"validation\"]), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f57f88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    12092\n",
      "1     5668\n",
      "Name: count, dtype: int64\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "CFG = {\n",
    "    'lr': 2e-5,\n",
    "    'epochs': 30,\n",
    "    'batch_size': 8,\n",
    "    'max_length': 200,\n",
    "    'min_length': 1,\n",
    "    'task': 'Sentiment',\n",
    "    'variety': 'en-UK',\n",
    "    'source': 'Google',\n",
    "    'model_name': 'bert-base-uncased'\n",
    "}\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(root_folder, splits['train']))\n",
    "labels_count = df_train[\"label\"].value_counts().sort_index()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(labels_count)\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9a7b59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Training Loss: 0.2813\n",
      "Epoch 2/30\n",
      "Training Loss: 0.0773\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     47\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 49\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     51\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_ds)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert_model_name = CFG['model_name']\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "# load classifier model\n",
    "model = transformers.BertForSequenceClassification.from_pretrained(\n",
    "    bert_model_name,\n",
    "    num_labels=2\n",
    ").to(device)\n",
    "\n",
    "train_ds = dataset_besstie.BesstieDataSet(\n",
    "    root_folder=root_folder,\n",
    "    file_name=splits['train'],\n",
    "    classes=['0', '1'],\n",
    "    tokenizer=tokenizer,\n",
    "    min_length=CFG['min_length'],\n",
    "    max_length=CFG['max_length'],\n",
    "    variety=CFG['variety'],\n",
    "    source=CFG['source'],\n",
    "    task=CFG['task']\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG['lr'])\n",
    "criterion = torch.nn.CrossEntropyLoss(\n",
    "    weight=torch.tensor(labels_count.values/sum(labels_count), dtype=torch.float).to(device)\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=CFG['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "for epoch in range(CFG['epochs']):\n",
    "    train_loss = 0.0\n",
    "    print(f\"Epoch {epoch+1}/{CFG['epochs']}\")\n",
    "    for batch in train_loader:\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device)\n",
    "        }\n",
    "        local_labels = batch['label'].tolist()\n",
    "        outputs = model(**inputs, labels=torch.tensor(local_labels).to(device))\n",
    "        loss = criterion(outputs.logits, torch.tensor(local_labels).to(device))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item() * inputs['input_ids'].size(0)\n",
    "\n",
    "    epoch_loss = train_loss / len(train_ds)\n",
    "    print(f\"Training Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef757b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
