{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3fb6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import transformers\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dataset.besstie import dataset_besstie\n",
    "\n",
    "root_folder = \"dataset/besstie/\"\n",
    "splits = {'train': 'train.csv', 'validation': 'valid.csv'}\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "if not os.path.exists(os.path.join(root_folder, splits[\"train\"])) or not os.path.exists(os.path.join(root_folder, splits[\"validation\"])):\n",
    "    print(\"Downloading BESSTIE dataset...\")\n",
    "    # Login using e.g. `huggingface-cli login` to access this dataset\n",
    "    df = pd.read_csv(\"hf://datasets/unswnlporg/BESSTIE/\" + splits[\"train\"])\n",
    "    df.to_csv(os.path.join(root_folder, splits[\"train\"]), index=False)\n",
    "    df = pd.read_csv(\"hf://datasets/unswnlporg/BESSTIE/\" + splits[\"validation\"])\n",
    "    df.to_csv(os.path.join(root_folder, splits[\"validation\"]), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "470d7461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f57f88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    12092\n",
      "1     5668\n",
      "Name: count, dtype: int64\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#TODO: add ability to choose validation subcategory of the dataset\n",
    "dataset_CFG = {\n",
    "    'dataset_name': 'BESSTIE',\n",
    "    'task': 'Sentiment',\n",
    "    'variety': 'en-UK',\n",
    "    'source': 'Google',\n",
    "}\n",
    "CFG = {\n",
    "    'lr': 2e-5,\n",
    "    'epochs': 8,\n",
    "    'batch_size': 8,\n",
    "    'max_length': 200,\n",
    "    'min_length': 1,\n",
    "    **dataset_CFG,\n",
    "    'model_name': 'bert-base-uncased',\n",
    "    'classification_head': 'conv', # 'linear' or 'conv' or 'lstm'\n",
    "    'seed': 0,\n",
    "}\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(root_folder, splits['train']))\n",
    "labels_count = df_train[\"label\"].value_counts().sort_index()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(labels_count)\n",
    "print(\"Using device:\", device)\n",
    "set_seed(CFG['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiKernelConvHead(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        num_labels: int,\n",
    "        kernel_sizes=(2, 3, 5),\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList([\n",
    "            torch.nn.Conv1d(\n",
    "                in_channels=input_size,\n",
    "                out_channels=hidden_size,\n",
    "                kernel_size=k,\n",
    "                padding=k // 2\n",
    "            )\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.pool = torch.nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.classifier = torch.nn.Linear(\n",
    "            hidden_size * len(kernel_sizes),\n",
    "            num_labels\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, H, L)\n",
    "        conv_outputs = []\n",
    "\n",
    "        for conv in self.convs:\n",
    "            h = self.activation(conv(x))      # (B, C, L)\n",
    "            h = self.pool(h).squeeze(-1)       # (B, C)\n",
    "            conv_outputs.append(h)\n",
    "\n",
    "        x = torch.cat(conv_outputs, dim=1)    # (B, C * num_kernels)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "def get_tokenizer_and_model(model_name:str):\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "    model = transformers.AutoModel.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "def get_classification_head(method: str, input_size:int, hidden_size: int, num_labels: int):\n",
    "    if method == \"linear\":\n",
    "        return torch.nn.Linear(input_size, num_labels)\n",
    "    elif method == \"conv\":\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(\n",
    "                in_channels=input_size,\n",
    "                out_channels=hidden_size,\n",
    "                kernel_size=3,\n",
    "                padding=1\n",
    "            ),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AdaptiveAvgPool1d(1),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(hidden_size, num_labels)\n",
    "        )\n",
    "    elif method == \"lstm\":\n",
    "        return torch.nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "    elif method == \"multi_conv\":\n",
    "        return MultiKernelConvHead(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_labels=num_labels,\n",
    "            kernel_sizes=(2, 3, 5),\n",
    "            num_channels=128,\n",
    "            dropout=0.1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown classification head method: {method}\")\n",
    "\n",
    "\n",
    "class MyClassifier(torch.nn.Module):\n",
    "    def __init__(self, base_model_name, classification_head_name, num_labels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer, self.base_model = get_tokenizer_and_model(base_model_name)\n",
    "        self.hidden_size = self.base_model.config.hidden_size\n",
    "        self.dropout = torch.nn.Dropout(self.base_model.config.hidden_dropout_prob)\n",
    "\n",
    "        self.classification_head_name = classification_head_name\n",
    "\n",
    "        self.classification_head = get_classification_head(\n",
    "            classification_head_name, self.hidden_size, num_labels\n",
    "        )\n",
    "\n",
    "        if classification_head_name == \"lstm\":\n",
    "            self.output_layer = torch.nn.Linear(self.hidden_size//2, num_labels)\n",
    "    \n",
    "    def get_tokenizer(self) -> transformers.PreTrainedTokenizer:\n",
    "        return self.tokenizer\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.base_model(**inputs)\n",
    "        sequence = self.dropout(outputs.last_hidden_state)\n",
    "\n",
    "        if self.classification_head_name == \"linear\":\n",
    "            cls_rep = sequence[:, 0, :]\n",
    "            logits = self.classification_head(cls_rep)\n",
    "\n",
    "        elif self.classification_head_name == \"conv\":\n",
    "            x = sequence.transpose(1, 2)\n",
    "            logits = self.classification_head(x)\n",
    "\n",
    "        elif self.classification_head_name == \"lstm\":\n",
    "            lstm_out, _ = self.classification_head(sequence)\n",
    "            cls_rep = lstm_out[:, 0, :]\n",
    "            logits = self.output_layer(cls_rep)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc87a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    for batch in val_loader:\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device)\n",
    "        }\n",
    "        local_labels = batch['label'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, local_labels)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        val_acc += torch.sum(preds == local_labels).item()\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader), val_acc / len(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47a57619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Test get classification head conv</strong> at: <a href='https://wandb.ai/elena-nespolo02-politecnico-di-torino/Figurative%20Analysis/runs/p65tl65y' target=\"_blank\">https://wandb.ai/elena-nespolo02-politecnico-di-torino/Figurative%20Analysis/runs/p65tl65y</a><br> View project at: <a href='https://wandb.ai/elena-nespolo02-politecnico-di-torino/Figurative%20Analysis' target=\"_blank\">https://wandb.ai/elena-nespolo02-politecnico-di-torino/Figurative%20Analysis</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251226_154236-p65tl65y\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Romeo\\Scuola\\Polito\\Anno5\\Quinto_anno\\Deep_natural_language_processing\\Sentiment_sarcasm\\Sentiment_Sarcasm_Analysis\\wandb\\run-20251226_164126-be7ng38s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/elena-nespolo02-politecnico-di-torino/Figurative%20Analysis/runs/be7ng38s' target=\"_blank\">Test get classification head conv</a></strong> to <a href='https://wandb.ai/elena-nespolo02-politecnico-di-torino/Figurative%20Analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/elena-nespolo02-politecnico-di-torino/Figurative%20Analysis' target=\"_blank\">https://wandb.ai/elena-nespolo02-politecnico-di-torino/Figurative%20Analysis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/elena-nespolo02-politecnico-di-torino/Figurative%20Analysis/runs/be7ng38s' target=\"_blank\">https://wandb.ai/elena-nespolo02-politecnico-di-torino/Figurative%20Analysis/runs/be7ng38s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "run_name = \"Test get classification head conv\"\n",
    "# run_name = None\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=\"elena-nespolo02-politecnico-di-torino\",\n",
    "    project=\"Figurative Analysis\",\n",
    "    name=run_name,\n",
    "    config=CFG,\n",
    "    tags=[CFG['dataset_name'], CFG['task'], CFG['model_name']]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a7b59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyClassifier(\n",
      "  (base_model): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classification_head): Sequential(\n",
      "    (0): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): AdaptiveAvgPool1d(output_size=1)\n",
      "    (3): Flatten(start_dim=1, end_dim=-1)\n",
      "    (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 228/228 [18:32<00:00,  4.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2450\n",
      "Epoch 2/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 228/228 [18:50<00:00,  4.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0760\n",
      "Epoch 3/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5/228 [00:29<22:16,  5.99s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 93\u001b[0m\n\u001b[0;32m     90\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     91\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, local_labels)\n\u001b[1;32m---> 93\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     95\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\python\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "model_name = CFG['model_name']\n",
    "\n",
    "tokenizer, model = get_tokenizer_and_model(model_name)\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# load classifier model\n",
    "# model = transformers.BertForSequenceClassification.from_pretrained(\n",
    "#     model_name,\n",
    "#     num_labels=2\n",
    "# ).to(device)\n",
    "\n",
    "model = MyClassifier(\n",
    "    base_model_name=model_name,\n",
    "    classification_head_name=CFG['classification_head'],\n",
    "    num_labels=2\n",
    ").to(device)\n",
    "\n",
    "train_ds = dataset_besstie.BesstieDataSet(\n",
    "    root_folder=root_folder,\n",
    "    file_name=splits['train'],\n",
    "    classes=['0', '1'],\n",
    "    tokenizer=tokenizer,\n",
    "    min_length=CFG['min_length'],\n",
    "    max_length=CFG['max_length'],\n",
    "    variety=CFG['variety'],\n",
    "    source=CFG['source'],\n",
    "    task=CFG['task']\n",
    ")\n",
    "\n",
    "val_ds = dataset_besstie.BesstieDataSet(\n",
    "    root_folder=root_folder,\n",
    "    file_name=splits['validation'],\n",
    "    classes=['0', '1'],\n",
    "    tokenizer=tokenizer,\n",
    "    min_length=CFG['min_length'],\n",
    "    max_length=CFG['max_length'],\n",
    "    variety=CFG['variety'],\n",
    "    source=CFG['source'],\n",
    "    task=CFG['task']\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr'])\n",
    "criterion = torch.nn.CrossEntropyLoss(\n",
    "    weight=torch.tensor(labels_count.values/sum(labels_count), dtype=torch.float).to(device)\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=CFG['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=CFG['batch_size'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "#TODO: gradient accumulation to reduce memory usage?\n",
    "# accumulation_steps = 4  # Effective batch size = batch_size * accumulation_steps\n",
    "# for i, batch in enumerate(train_dataloader):\n",
    "#     outputs = model(**batch)\n",
    "#     loss = outputs.loss / accumulation_steps\n",
    "#     loss.backward()\n",
    "#     if (i + 1) % accumulation_steps == 0:\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "#         model.zero_grad()\n",
    "\n",
    "for epoch in range(CFG['epochs']):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    print(f\"Epoch {epoch+1}/{CFG['epochs']}\")\n",
    "    \n",
    "    pbar = tqdm.tqdm(train_loader)\n",
    "    for batch in pbar:\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device)\n",
    "        }\n",
    "        local_labels = batch['label'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, local_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    epoch_loss = train_loss / len(train_loader)\n",
    "    run.log({\"train_loss\": epoch_loss, \"val_loss\": val_loss, \"val_acc\": val_acc})\n",
    "    print(f\"Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7ef757b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▁</td></tr><tr><td>val_acc</td><td>▁█</td></tr><tr><td>val_loss</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>0.076</td></tr><tr><td>val_acc</td><td>0.95968</td></tr><tr><td>val_loss</td><td>0.23824</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Test get classification head conv</strong> at: <a href='https://wandb.ai/elena-nespolo02-politecnico-di-torino/Figurative%20Analysis/runs/be7ng38s' target=\"_blank\">https://wandb.ai/elena-nespolo02-politecnico-di-torino/Figurative%20Analysis/runs/be7ng38s</a><br> View project at: <a href='https://wandb.ai/elena-nespolo02-politecnico-di-torino/Figurative%20Analysis' target=\"_blank\">https://wandb.ai/elena-nespolo02-politecnico-di-torino/Figurative%20Analysis</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251226_164126-be7ng38s\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
